\textbf{Hypothesis 1: Are prices lower due to supply-side effects?}

Market discrimination laid out by Becker rests on the idea that groups who are discriminated against see lower demand in the market, which drives down their prices. I have estimated that prices are lower, but there could be multiple explanations in addition to discrimination for this result. One hypothesis is that, for the same type of listing, minority hosts \textit{choose} to price their listings lower than white hosts. This might happen if minority hosts earn lower wages because they experience discrimination in the labor market. A lower opportunity cost of time would mean they have a lower marginal cost of putting up and managing their listing than a white host who owns a similar property. If minority hosts choose to price lower for every quantity, this is, in effect, equivalent to the supply curve being lower for minority hosts relative to white hosts. We therefore know that we can test this hypothesis by looking at the quantity demanded. If prices are low because the supply curve is lower, then minority hosts would have a higher quantity demanded. Conversely, if the price are low because the demand curve is lower - which would be in line with the presence of discrimination - then the quantity demanded should be lower than it is for white hosts. 

In order to test this hypothesis, I use number of reviews as a proxy for quantity demanded. In Table 6, I regress the number of reviews on host race, controlling for the same set of models as Table 3. I find that minority hosts have either the same or lower review numbers than white hosts for a similar listing that spent the same amount of time on the market. Most coefficients are either roughly zero, or negative and in the range of 1-2 reviews less than white hosts. The results are significant for white females and black hosts. While the coefficients were significant for Asian hosts under the less robust specifications, under the full specification the coefficient is not significant, but still slightly negative. In general, my results suggest that non-white male hosts do not see a higher quantity demanded than white male hosts. This is evidence against the supply-side explanation. 

It is important to keep in mind that this conclusion is only salient if the total number of reviews is a reasonable proxy for the demand of a listing. Yet, one can imagine that if reviewers systematically under-review minority hosts relative to white hosts, these groups would have lower numbers of reviews that do not necessarily represent a lower quantity demanded. There is no way to tell apart these mechanisms in my data, and no previous research has been done on race-based differences in review rates. 

My working assumption is that even if not every guest leaves a review, the review proportion is similar across host race, and a lower number of reviews therefore indicates a real difference between quantity demanded of minority hosts and white hosts. I substantiate this assumption with another supply-side metric, a measure of listing vacancy that I explore in the next section, that provides more evidence that lower prices are not driven by supply-side effects. Taken together, these two measures will provide strong evidence to reject the hypothesis that lower prices are due to supply-side effects. 
    
    
\textbf{Hypothesis 2: Are number of reviews lower because minority hosts offer their listings a fewer number of days?}

In the previous section, I argued that minority hosts had a lower quantity demanded than comparable white hosts. However, they may have lower number of guests because they offer up their listing for fewer days of the month, not because people don't want to stay with minority hosts. In order to test this, I regress the availability of the listing out of 30 days on host race, controlling for my preferred specification. The availability of a listing is controlled by the host, who can update their availability calendar on their listing page. Potential guests can then see on which days the listing is available and book accordingly. When a guest books an available day, that day is removed from the availability calendar. Therefore, the availability out of 30 days measure is a true measure of the \textit{vacancy} rate of a listing.

The results, presented in Table 7, are striking. I find that the listings of black hosts spend about 20\% more time on the market vacant than the listings of white males. The effect is statistically significant, and amounts to about 2-3 days per month in real units. Interestingly, white females make their listing less available than white males, with approximately a .9 of a day statistically significant difference. This might explain why white females don't have lower prices than white males, but did have lower numbers of reviews. Perhaps white females simply offer their listing for fewer days than their white male counterparts. There were no statistically significant effects in availability for Hispanic hosts, like for most of my measures. Asian female hosts actually had a lower vacancy rate than white male hosts, which could help explain why they have a lower number of reviews. 

Overall, there is strong evidence that even though black hosts offer their listing for more days, they have less people staying with them than white hosts. This is significant evidence to reject the supply-side hypothesis for black hosts. This is not the case for Asian hosts - the coefficients for Asian hosts were negative, indicating that they actually are putting up their listing for rent less often than white hosts. This means that lower availability is another possible explanation, in addition to discrimination, for why Asian hosts have a lower number of reviews. However, I am not able to further distinguish between these two hypotheses in my data.  

\textbf{Hypothesis 3: Are their prices lower because minority hosts have worse reviews?} 

Reviews are often critical for the decisions guests make about the listings they book. It is reasonable to expect the quality of a listing's reviews influences the demand, and therefore the price, for that listing. Previous analyses, including Edelman and Luca (2014), involved controlling for the numeric review score of the listing as a proxy for listing ``quality". However, the numeric review score on a listing often carries little information about the real quality of the listing because there is very little variation in the numeric score.\footnote{A low share of guests who review may be a more accurate proxy for low quality, because many users prefer to leave no review rather than a negative review. Review share information, however, is not available, so instead I use Sentimentr to measure quality from reviews.} In my data, 50\% of listings had an average review of $>$ 96 out of 100, and 75\% had an average review score above 91 out of 100.\footnote{This is the case for most online marketplaces. Fradkin, Grewal, and Holtz (2017) study the determinants of review informativeness on Airbnb and find that most reviews, both numeric and text, are positive. In general, reviews tend to reflect real experience of the user.\cite{fradkin}} An Airbnb guest, seeing little variation in the number of stars different hosts have, may instead rely on the text of the reviews to make their booking decision. Since review text allows guests more flexibility in the feedback they give, it may provide a more accurate and nuanced picture of the guest's experience.\footnote{This is because a guest who leaves a text review have the opportunity to use qualifiers like ``but", or ``except", strengthening words like ``really" or ``a lot", etc.} For this reason I use review text instead of the numeric score in the analysis. 

In order to do this, the race, sex, and age of 16,000 reviewers who left reviews for a subset of the Chicago hosts was coded.\footnote{16,000 is 23\% of the total amount of Chicago reviewers in the data set} For each sentence of each review, I used a sentiment-analysis algorithm called Sentimentr to evaluate how positive or negative the sentence is. Sentimentr uses a dictionary of positive and negative words to assign each sentence a sentiment score from -1 to 1, where 1 is a positive sentence, -1 is a negative sentence, and 0 is a neutral sentence carrying no emotion. Unlike other sentiment analysis programs, Sentimentr doesn't merely count the number of good or bad words in a sentence. It also takes into account valence shifters, or words that affect the sentiment-carrying word in the sentence. For example, the algorithm assigns ``I like the listing", ``I \textit{really} like the listing", and ``I like the listing, \textit{but}..." different valence scores because of the presence of valence-shifting words like ``really" and ``but". Sentimentr calculates the correct sentiment 60-70\% of the time as compared to a human grader. One limitation of conducting sentiment analysis in this way, however, is that not every review that a human would consider bad or good carries a sentiment word that the algorithm would pick up. For example, ``The apartment had cockroaches" is certainly a horrible review, but would be given a score of 0 by Sentimentr because it contains no emotion-laden words. 

In Table 8, I regress this sentiment score on the host race, controlling for my preferred specification from Model 4 in Table 3. This means that each coefficient should be read as the standardized review quality, relative to white males, that reviewer of type A gave host of type B.\footnote{Review quality was standardized with mean 0 and standard deviation of 1.} I break up my regressions by the race and sex of the reviewer, varying across the columns of Table 8. The race and sex of the host varies by row. I therefore am able to see any trends in the quality of the review different types of reviewers gave to different types of hosts. 

I find that results were mixed. Overall, white reviewers show little evidence of systematic bias against minority hosts, as measured by the sentiment scores provided by Sentimentr. The reviews that white guests leave for minority hosts do not significantly differ in quality from those they leave for white male hosts. There are stronger effects when considering the quality of the review minority reviewers gave to minority hosts. Black male guests rate Asian hosts almost 4-8 standard deviations above the mean, but rate black women 3 standard deviation lower than the mean. All minority female reviewers, including black females, rate black men worse than they would rate white men who own a similar type of listing. However, all minority male reviewers rate black men anywhere from .5-2 standard deviations higher than they do white men. This suggests that there is some gender-based favoritism between minority reviewers and black male hosts. However, it is important to keep in mind that some of these large, very significant coefficients are suspicious because of small sample sizes - in several thousand Chicago host and reviewer pairs, there are simply not enough black men who stayed with Asian women to be representative of the overall distribution.

In general, there is no one minority group that uniformly has lower quality reviews. Some groups do tend to give other groups far better reviews, but there is no larger pattern of within-gender or within-race bias between hosts and guests that holds for more than one host-guest pair. Overall, there is not enough evidence to substantiate that minority hosts have systematically lower review quality that can explain lower prices. 
